{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: C:/Users/Rohit/testspeaker.wav\n",
      "\tdetected as -  anthonyschaller\n",
      "\n",
      "Wall time: 11min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog \n",
    "import tkinter.messagebox as tm\n",
    "from tkinter import *\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from spacy.lang.en import English\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.summarization import summarize\n",
    "from gensim.summarization import keywords\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "import gensim\n",
    "import pyLDAvis.gensim\n",
    "import re\n",
    "import speech_recognition as sr\n",
    "import subprocess\n",
    "import uuid\n",
    "import os\n",
    "import _pickle as cPickle\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "import python_speech_features as mfcc\n",
    "from sklearn.mixture import GMM\n",
    "from punctuator import Punctuator\n",
    "p = Punctuator('Demo-Europarl-EN.pcl')\n",
    "\n",
    "datafile=\"\"\n",
    "topicfile=\"\"\n",
    "speakerfile=\"\"\n",
    "parser = English()\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens\n",
    "\n",
    "\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "\n",
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def SpeakerUpload(event=None):\n",
    "    global speakerfile\n",
    "\n",
    "    def calculate_delta(array):\n",
    "        \"\"\"Calculate and returns the delta of given feature vector matrix\"\"\"\n",
    "\n",
    "        rows,cols = array.shape\n",
    "        deltas = np.zeros((rows,20))\n",
    "        N = 2\n",
    "        for i in range(rows):\n",
    "            index = []\n",
    "            j = 1\n",
    "            while j <= N:\n",
    "                if i-j < 0:\n",
    "                    first = 0\n",
    "                else:\n",
    "                    first = i-j\n",
    "                if i+j > rows -1:\n",
    "                    second = rows -1\n",
    "                else:\n",
    "                    second = i+j\n",
    "                index.append((second,first))\n",
    "                j+=1\n",
    "            deltas[i] = ( array[index[0][0]]-array[index[0][1]] + (2 * (array[index[1][0]]-array[index[1][1]])) ) / 10\n",
    "        return deltas\n",
    "\n",
    "    def extract_features(audio,rate):\n",
    "        \"\"\"extract 20 dim mfcc features from an audio, performs CMS and combines \n",
    "        delta to make it 40 dim feature vector\"\"\"    \n",
    "    \n",
    "        mfcc_feat = mfcc.mfcc(audio,rate, 0.025, 0.01,20,appendEnergy = True)\n",
    "    \n",
    "        mfcc_feat = preprocessing.scale(mfcc_feat)\n",
    "        delta = calculate_delta(mfcc_feat)\n",
    "        combined = np.hstack((mfcc_feat,delta)) \n",
    "        return combined\n",
    "\n",
    "\n",
    "\n",
    "    pathspeaker=filedialog.askopenfilename()\n",
    "    print('Selected:',pathspeaker)\n",
    "#path to training data\n",
    "    source   = \"development_set\\\\\"   \n",
    "\n",
    "#path where training speakers will be saved\n",
    "    dest = \"speaker_models\\\\\"\n",
    "\n",
    "    train_file = \"development_set_enroll.txt\"        \n",
    "\n",
    "\n",
    "    file_paths = open(train_file,'r')\n",
    "\n",
    "    count = 1\n",
    "\n",
    "# Extracting features for each speaker (5 files per speakers)\n",
    "    features = np.asarray(())\n",
    "    for path in file_paths:    \n",
    "        path = path.strip()   \n",
    "    \n",
    "    # read the audio\n",
    "        sr,audio = read(source + path)\n",
    "    \n",
    "    # extract 40 dimensional MFCC & delta MFCC features\n",
    "        vector   = extract_features(audio,sr)\n",
    "    \n",
    "        if features.size == 0:\n",
    "            features = vector\n",
    "        else:\n",
    "            features = np.vstack((features, vector))\n",
    "    # when features of 5 files of speaker are concatenated, then do model training\n",
    "        if count == 5:    \n",
    "            gmm = GMM(n_components = 16, n_iter = 200, covariance_type='diag',n_init = 3)\n",
    "            gmm.fit(features)\n",
    "        \n",
    "        # dumping the trained gaussian model\n",
    "            picklefile = path.split(\"-\")[0]+\".gmm\"\n",
    "            cPickle.dump(gmm,open(dest + picklefile,'wb'))\n",
    "  \n",
    "            features = np.asarray(())\n",
    "            count = 0\n",
    "        count = count + 1\n",
    "    \n",
    "    #test_gender.py\n",
    "\n",
    "\n",
    "\n",
    "#path to training data\n",
    "    source   = \"development_set\\\\\"   \n",
    "\n",
    "    modelpath = \"speaker_models\\\\\"\n",
    "\n",
    "\n",
    "    gmm_files = [os.path.join(modelpath,fname) for fname in \n",
    "                  os.listdir(modelpath) if fname.endswith('.gmm')]\n",
    "\n",
    "#Load the Gaussian gender Models\n",
    "    models    = [cPickle.load(open(fname,'rb')) for fname in gmm_files]\n",
    "    speakers   = [fname.split(\"\\\\\")[-1].split(\".gmm\")[0] for fname \n",
    "                  in gmm_files]\n",
    "\n",
    "# Read the test directory and get the list of test audio files \n",
    "    #pathspeaker=filedialog.askopenfilename()\n",
    "    #print('Selected:',pathspeaker)   \n",
    "    sr,audio = read(pathspeaker)\n",
    "    vector   = extract_features(audio,sr)\n",
    "    log_likelihood = np.zeros(len(models)) \n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        gmm    = models[i]         #checking with each model one by one\n",
    "        scores = np.array(gmm.score(vector))\n",
    "        log_likelihood[i] = scores.sum()\n",
    "    \n",
    "    winner = np.argmax(log_likelihood)\n",
    "    speakerfile=speakers[winner]\n",
    "    print(\"\\tdetected as - \",speakers[winner])\n",
    "    time.sleep(1.0)\n",
    "\n",
    "def AudioUpload(event=None):\n",
    "    global datafile\n",
    "    global topicfile\n",
    "    global datafile1\n",
    "    \n",
    "    filename = filedialog.askopenfilename()\n",
    "    print('Selected:', filename)\n",
    "    filename1=\"audio.wav\"\n",
    "    command = [r\"C:\\Users\\Rohit\\Downloads\\ffmpeg-20200403-52523b6-win64-static\\ffmpeg-20200403-52523b6-win64-static\\bin\\ffmpeg.exe\",\"-y\",\"-i\", filename, filename1] #-y is used so that you can overwrite the audio file\n",
    "    subprocess.run(command,shell=True)\n",
    "    r = sr.Recognizer()\n",
    "    \n",
    "    with sr.AudioFile(filename1) as source:\n",
    "        r.adjust_for_ambient_noise(source)\n",
    "        audio = r.record(source)\n",
    "    try:\n",
    "        datafile1=r.recognize_sphinx(audio)\n",
    "        print(type(datafile1))\n",
    "        print(\"Sphinx thinks you said \" + datafile1)\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Sphinx could not understand audio\")\n",
    "    except sr.RequestError as e:\n",
    "        print(\"Sphinx error; {0}\".format(e))\n",
    "    \n",
    "    unique_filename = str(uuid.uuid4())\n",
    "    print(unique_filename)\n",
    "    f = open(unique_filename+\".txt\", \"w+\",encoding='utf-8')\n",
    "    print(type(datafile1))\n",
    "    v=f.write(datafile1)\n",
    "    f.close()\n",
    "    \n",
    "    text_data = []\n",
    "    summ=[]\n",
    "    f = open(unique_filename+\".txt\", \"r\",encoding='utf-8')\n",
    "    for line in f:\n",
    "        m=p.punctuate(line)\n",
    "        summ.append(summarize(m,ratio=0.05))\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        if True:\n",
    "            #print(tokens)\n",
    "            text_data.append(tokens) \n",
    "    datafile=summ\n",
    "    dictionary = corpora.Dictionary(text_data)\n",
    "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "    pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "    dictionary.save('dictionary.gensim')\n",
    "    \n",
    "\n",
    "    NUM_TOPICS = 2\n",
    "    dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "    corpus = pickle.load(open('corpus.pkl', 'rb'))\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "    ldamodel.save('model3.gensim')\n",
    "    topicfile = ldamodel.print_topics(num_words=5)\n",
    "    print(topicfile)\n",
    "\n",
    "    lda = gensim.models.ldamodel.LdaModel.load('model3.gensim')\n",
    "    \n",
    "    # to extract words from string \n",
    "    # using regex( findall() ) \n",
    "  \n",
    "    test_string = topicfile[0][1]\n",
    "\n",
    "    # using regex( findall() ) \n",
    "    # to extract words from string \n",
    "    res = re.findall(r'\\w+', test_string) \n",
    "\n",
    "    # Python program to Remove all digits from a list of string \n",
    "    from string import digits \n",
    "\n",
    "    def remove(listr): \n",
    "        listr = [''.join(x for x in i if x.isalpha()) for i in listr] \n",
    "        return listr\n",
    "\n",
    "    # Driver code \n",
    "\n",
    "    listr = res\n",
    "    v=remove(listr)\n",
    "    #print(v)\n",
    "\n",
    "    # Function to convert \n",
    "    def listToString(s): \n",
    "        str1 = \"\" \n",
    "    \n",
    "    # traverse in the string \n",
    "        for ele in s: \n",
    "            str1 += ele+' '  \n",
    "        return str1 \n",
    "\n",
    "    # Driver code\t \n",
    "    s = v\n",
    "    topicfile=listToString(s)\n",
    "    print(topicfile)\n",
    "    \n",
    "    print(datafile[0])\n",
    "    #lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "    #pyLDAvis.display(lda_display)\n",
    "\n",
    "def UploadAction(event=None):\n",
    "    global datafile\n",
    "    global topicfile\n",
    "    filename = filedialog.askopenfilename()\n",
    "    print('Selected:', filename) \n",
    "    \n",
    "    text_data = []\n",
    "    summ=[]\n",
    "    with open(filename, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            print(\"Hello\",f)\n",
    "            summ.append(summarize(line,ratio=0.05))\n",
    "            tokens = prepare_text_for_lda(line)\n",
    "            if True:\n",
    "                #print(tokens)\n",
    "                text_data.append(tokens)\n",
    "    datafile=summ         \n",
    "    dictionary = corpora.Dictionary(text_data)\n",
    "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "    pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "    dictionary.save('dictionary.gensim')\n",
    "    \n",
    "\n",
    "    NUM_TOPICS = 2\n",
    "    dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "    corpus = pickle.load(open('corpus.pkl', 'rb'))\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "    ldamodel.save('model3.gensim')\n",
    "    topicfile = ldamodel.print_topics(num_words=5)\n",
    "    print(topicfile)\n",
    "\n",
    "    lda = gensim.models.ldamodel.LdaModel.load('model3.gensim')\n",
    "    \n",
    "    # to extract words from string using regex( findall() )\n",
    "    test_string = topicfile[0][1]\n",
    "\n",
    "    # using regex( findall() ) to extract words from string \n",
    "    res = re.findall(r'\\w+', test_string) \n",
    "\n",
    " \n",
    "    # digits from a list of string \n",
    "    from string import digits \n",
    "\n",
    "    def remove(listr): \n",
    "        listr = [''.join(x for x in i if x.isalpha()) for i in listr] \n",
    "        return listr\n",
    "\n",
    "    # Driver code \n",
    "\n",
    "    listr = res\n",
    "    v=remove(listr)\n",
    "    #print(v)\n",
    "\n",
    "    # Function to convert \n",
    "    def listToString(s): \n",
    "\n",
    "    # initialize an empty string \n",
    "        str1 = \"\" \n",
    "    \n",
    "    # traverse in the string \n",
    "        for ele in s: \n",
    "            str1 += ele+' ' \n",
    "        # return string \n",
    "        return str1 \n",
    "\n",
    "    # Driver code\t \n",
    "    s = v\n",
    "    topicfile=listToString(s) \n",
    "    print(topicfile)\n",
    "    \n",
    "    print(datafile[0])\n",
    "    #lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "    #pyLDAvis.display(lda_display)\n",
    "    \n",
    "def Display_function():\n",
    "    global topicfile\n",
    "    print(topicfile)\n",
    "    \n",
    "    textBox2.delete('1.0', END)\n",
    "    textBox2.insert(tk.END, topicfile)\n",
    "\n",
    "def Summary_function():\n",
    "    global datafile\n",
    "    print(datafile)\n",
    "    \n",
    "    textBox1.delete('1.0', END)\n",
    "    textBox1.insert(tk.END, datafile[0])\n",
    "\n",
    "def SpeakerDisplay():\n",
    "    global datafile\n",
    "    print(datafile)\n",
    "    \n",
    "    textBox3.delete('1.0', END)\n",
    "    textBox3.insert(tk.END, speakerfile)\n",
    "\n",
    "window = tk.Tk()\n",
    "#window.geometry(\"200x200+30+30\")\n",
    "window.state('zoomed')\n",
    "window.title(\"MINUTES of MEETING PROTOTYPE\")\n",
    "button1 = tk.Button(window, text='Upload Text File', command=UploadAction)\n",
    "button4 = tk.Button(window, text='Upload Audio/Video\\nFile', command=AudioUpload)\n",
    "button5 = tk.Button(window, text='Upload Speaker', command=SpeakerUpload)\n",
    "button2 = tk.Button(window, text='Keywords', command=Display_function)\n",
    "button3 = tk.Button(window, text='Summarize', command=Summary_function)\n",
    "button6 = tk.Button(window, text='Speaker', command=SpeakerDisplay)\n",
    "\n",
    "#keywords\n",
    "display = tk.Label(window)\n",
    "textBox2=Text(window, height=10, width=30)\n",
    "display.pack()\n",
    "o = tk.Label(window, text=\"Relevant Keywords\")\n",
    "o.pack()\n",
    "o.place(x = 360, y = 40, width=120, height=50)\n",
    "textBox2.pack()\n",
    "textBox2.place(x = 300, y = 90)\n",
    "\n",
    "#summary\n",
    "display = tk.Label(window)\n",
    "textBox1=Text(window, height=10, width=80)\n",
    "display.pack()\n",
    "o = tk.Label(window, text=\"Summary of the topic\")\n",
    "o.pack()\n",
    "o.place(x = 850, y = 40, width=120, height=50)\n",
    "textBox1.pack()\n",
    "textBox1.place(x = 620, y= 90)\n",
    "\n",
    "#speaker\n",
    "display = tk.Label(window)\n",
    "textBox3=Text(window, height=8, width=60)\n",
    "display.pack()\n",
    "o = tk.Label(window, text=\"Speaker Name\")\n",
    "o.pack()\n",
    "o.place(x = 880, y =400, width=120, height=50)\n",
    "textBox3.pack()\n",
    "textBox3.place(x = 700, y= 450)\n",
    "\n",
    "#upload text file\n",
    "button1.pack()\n",
    "button1.place(x = 70, y = 100, width=120, height=50)\n",
    "#keywords\n",
    "button2.pack()\n",
    "button2.place(x = 350, y = 280, width=120, height=50)\n",
    "#summarize\n",
    "button3.pack()\n",
    "button3.place(x = 860, y = 280, width=120, height=50)\n",
    "#upload Audio/Video\n",
    "button4.pack()\n",
    "button4.place(x= 70, y =180,width=120, height=80)\n",
    "o = tk.Label(window, text=\"Processing takes\\n2-3 minutes based\\n on Audio file size\")\n",
    "o.pack()\n",
    "o.place(x = 70, y = 270, width=120, height=50)\n",
    "#Upload Speaker file\n",
    "button5.pack()\n",
    "button5.place(x= 450, y =450,width=120, height=50)\n",
    "o = tk.Label(window, text=\"Processing takes\\n15-20 seconds\")\n",
    "o.pack()\n",
    "o.place(x = 450, y = 500, width=120, height=50)\n",
    "#speaker\n",
    "button6.pack()\n",
    "button6.place(x= 880, y =600,width=120, height=50)\n",
    "\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
